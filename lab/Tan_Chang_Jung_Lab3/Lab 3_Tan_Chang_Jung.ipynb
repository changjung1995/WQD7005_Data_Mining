{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lab1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(df):\n",
    "    df = df.drop(['TargetD', 'GiftCnt36', 'GiftCntAll', 'GiftCntCard36', 'GiftCntCardAll', 'GiftAvgLast', 'GiftAvg36','GiftAvgAll','GiftTimeLast','GiftTimeFirst'], axis=1)\n",
    "    \n",
    "    mask = pd.isnull(df['DemAge'])\n",
    "\n",
    "    df = df[~mask]\n",
    "    \n",
    "    df['DemAge'].fillna(df['DemAge'].mean(), inplace=True)\n",
    "    \n",
    "\n",
    "    df['DemMedIncome'] = pd.isnull(df['DemMedIncome'])\n",
    "    df['GiftAvgCard36'] = pd.isnull(df['GiftAvgCard36'])\n",
    "    df['DemMedIncome'].fillna(0, inplace=True)\n",
    "    df['GiftAvgCard36'].fillna(0, inplace=True)\n",
    "    \n",
    "    df = df.drop(['PromCnt12', 'PromCnt36','PromCntAll','PromCntCard12','PromCntCard36','PromCntCardAll'], axis=1) \n",
    "    df = df.dropna()\n",
    "    \n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7279 entries, 0 to 0\n",
      "Data columns (total 19 columns):\n",
      "ID                  7279 non-null int64\n",
      "GiftAvgCard36       7279 non-null bool\n",
      "StatusCatStarAll    7279 non-null int64\n",
      "DemCluster          7279 non-null int64\n",
      "DemAge              7279 non-null float64\n",
      "DemMedHomeValue     7279 non-null int64\n",
      "DemPctVeterans      7279 non-null int64\n",
      "DemMedIncome        7279 non-null bool\n",
      "StatusCat96NK_A     7279 non-null uint8\n",
      "StatusCat96NK_E     7279 non-null uint8\n",
      "StatusCat96NK_F     7279 non-null uint8\n",
      "StatusCat96NK_L     7279 non-null uint8\n",
      "StatusCat96NK_N     7279 non-null uint8\n",
      "StatusCat96NK_S     7279 non-null uint8\n",
      "DemGender_F         7279 non-null uint8\n",
      "DemGender_M         7279 non-null uint8\n",
      "DemGender_U         7279 non-null uint8\n",
      "DemHomeOwner_H      7279 non-null uint8\n",
      "DemHomeOwner_U      7279 non-null uint8\n",
      "dtypes: bool(2), float64(1), int64(5), uint8(11)\n",
      "memory usage: 490.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tan Chang Jung\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing\n",
    "X = df.drop(['GiftAvgCard36'], axis=1)\n",
    "y = df['GiftAvgCard36']\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Standardisation and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What is the difference between logistic regression and linear regression? \n",
    "\n",
    "Logistic regression is used to predicting dependent variables that are binary in class.\n",
    "Linear regression is used when the dependent variable is in continuous value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Describe how logistic regression perform its prediction\n",
    "\n",
    "Logistic regression computes the probability of an event occurrence by using a log of odds as the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write code to perform standardisation on your training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69402919, -1.12179409,  1.06177714, ..., -0.18635413,\n",
       "        -1.42195128,  1.42195128],\n",
       "       [-0.0070011 ,  0.89142919,  0.3013199 , ..., -0.18635413,\n",
       "         0.70325898, -0.70325898],\n",
       "       [ 1.67362395, -1.12179409,  1.61483695, ..., -0.18635413,\n",
       "        -1.42195128,  1.42195128],\n",
       "       ...,\n",
       "       [ 0.3590343 ,  0.89142919, -0.87393219, ..., -0.18635413,\n",
       "         0.70325898, -0.70325898],\n",
       "       [ 0.57430671,  0.89142919,  0.09392247, ..., -0.18635413,\n",
       "         0.70325898, -0.70325898],\n",
       "       [ 0.39406351, -1.12179409, -0.18260743, ..., -0.18635413,\n",
       "         0.70325898, -0.70325898]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.49818868,  0.89142919,  0.92351219, ..., -0.18635413,\n",
       "         0.70325898, -0.70325898],\n",
       "       [ 0.55632216,  0.89142919, -0.94306467, ...,  5.36612725,\n",
       "         0.70325898, -0.70325898],\n",
       "       [ 1.47196195,  0.89142919,  1.13090962, ..., -0.18635413,\n",
       "        -1.42195128,  1.42195128],\n",
       "       ...,\n",
       "       [-0.10860028, -1.12179409, -0.87393219, ..., -0.18635413,\n",
       "         0.70325898, -0.70325898],\n",
       "       [-0.76678078, -1.12179409,  1.26917457, ..., -0.18635413,\n",
       "         0.70325898, -0.70325898],\n",
       "       [ 1.11033683,  0.89142919, -0.94306467, ..., -0.18635413,\n",
       "         0.70325898, -0.70325898]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What does standardisation do to your data? How does it benefit your regression model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardisation scale input features with normal distribution mean of 0 and standard deviation of 1. Standardisation for regression models can reduce the time in optimizing the weights during gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Write code to fit a logistic regression model to your training data. How does it perform on the training and test data? Do you see any indication of overfitting?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0.8143277723258097\n",
      "Test: 0.8067765567765568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tan Chang Jung\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training:\", logreg.score(X_train, y_train))\n",
    "print(\"Test:\", logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training and testing accuracy are high and almost similar, implies that the model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Write code to find the most important features in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tan Chang Jung\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=0, solver='warn',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': [1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10,\n",
       "                               100, 1000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "# use all cores to tune logistic regression with C parameter\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=0), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StatusCat96NK_S : -0.00039595519485191333\n",
      "StatusCat96NK_A : 0.00036455221936939876\n",
      "StatusCatStarAll : -0.00029305025473820015\n",
      "StatusCat96NK_E : 0.0001669022052626906\n",
      "StatusCat96NK_N : -0.00012473109368537686\n",
      "StatusCat96NK_L : 9.422832714177853e-05\n",
      "DemPctVeterans : -7.218149157640623e-05\n",
      "DemAge : -4.588185191022331e-05\n",
      "DemGender_M : 4.035605359256606e-05\n",
      "DemGender_F : -3.1544269144665554e-05\n",
      "DemMedHomeValue : 2.5075159520180007e-05\n",
      "DemGender_U : -2.3395237073187303e-05\n",
      "DemCluster : 1.6201779797031502e-05\n",
      "ID : -7.047181436590488e-06\n",
      "StatusCat96NK_F : -6.213725640201492e-06\n",
      "DemHomeOwner_U : 7.546056781895748e-07\n",
      "DemHomeOwner_H : -7.546056781894461e-07\n",
      "DemMedIncome : 0.0\n"
     ]
    }
   ],
   "source": [
    "coef = cv.best_estimator_.coef_[0]\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(np.absolute(coef))\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StatusCat96NK_L consider as the most important feature, followed by DemHomeOwner_U and DemGender_M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
